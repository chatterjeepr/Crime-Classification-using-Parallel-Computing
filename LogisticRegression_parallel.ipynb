{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ab4671-5320-405a-878f-ff0738262d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/13 12:05:19 WARN Utils: Your hostname, Shalvis-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.4.76 instead (on interface en0)\n",
      "23/12/13 12:05:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/13 12:05:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"crime-classification\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c663bdb-08ff-4523-a5ac-4fbe7799bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "np.random.seed(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "278658db-361b-4cf0-b281-da644f57ad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Structure\n",
      "----------------------------------\n",
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n",
      "None\n",
      " \n",
      "Dataframe preview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|      Category|         Description|\n",
      "+--------------+--------------------+\n",
      "|      warrants|      warrant arrest|\n",
      "|other offenses|traffic violation...|\n",
      "|other offenses|traffic violation...|\n",
      "| larceny/theft|grand theft from ...|\n",
      "| larceny/theft|grand theft from ...|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      " \n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows 878049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Read the data into spark datafrome\n",
    "from pyspark.sql.functions import col, lower\n",
    "df = spark.read.format('csv')\\\n",
    "          .option('header','true')\\\n",
    "          .option('inferSchema', 'true')\\\n",
    "          .option('timestamp', 'true')\\\n",
    "          .load('./sf-crime/train.csv')\n",
    "\n",
    "data = df.select(lower(col('Category')),lower(col('Descript')))\\\n",
    "        .withColumnRenamed('lower(Category)','Category')\\\n",
    "        .withColumnRenamed('lower(Descript)', 'Description')\n",
    "data.cache()\n",
    "print('Dataframe Structure')\n",
    "print('----------------------------------')\n",
    "print(data.printSchema())\n",
    "print(' ')\n",
    "print('Dataframe preview')\n",
    "print(data.show(5))\n",
    "print(' ')\n",
    "print('----------------------------------')\n",
    "print('Total number of rows', df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40968952-565f-4bf6-8eea-1d681686a727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique value of Category: 39\n",
      " \n",
      "Top 10 Crime Category\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|      Category|totalValue|\n",
      "+--------------+----------+\n",
      "| larceny/theft|    174900|\n",
      "|other offenses|    126182|\n",
      "|  non-criminal|     92304|\n",
      "|       assault|     76876|\n",
      "| drug/narcotic|     53971|\n",
      "| vehicle theft|     53781|\n",
      "|     vandalism|     44725|\n",
      "|      warrants|     42214|\n",
      "|      burglary|     36755|\n",
      "|suspicious occ|     31414|\n",
      "+--------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique value of Description: 879\n",
      " \n",
      "Top 10 Crime Description\n",
      "+--------------------+----------+\n",
      "|         Description|totalValue|\n",
      "+--------------------+----------+\n",
      "|grand theft from ...|     60022|\n",
      "|       lost property|     31729|\n",
      "|             battery|     27441|\n",
      "|   stolen automobile|     26897|\n",
      "|drivers license, ...|     26839|\n",
      "|      warrant arrest|     23754|\n",
      "|suspicious occurr...|     21891|\n",
      "|aided case, menta...|     21497|\n",
      "|petty theft from ...|     19771|\n",
      "|malicious mischie...|     17789|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def top_n_list(df,var, N):\n",
    "    '''\n",
    "    This function determine the top N numbers of the list\n",
    "    '''\n",
    "    print(\"Total number of unique value of\"+' '+var+''+':'+' '+str(df.select(var).distinct().count()))\n",
    "    print(' ')\n",
    "    print('Top'+' '+str(N)+' '+'Crime'+' '+var)\n",
    "    df.groupBy(var).count().withColumnRenamed('count','totalValue')\\\n",
    "    .orderBy(col('totalValue').desc()).show(N)\n",
    "    \n",
    "    \n",
    "top_n_list(data, 'Category',10)\n",
    "print(' ')\n",
    "print(' ')\n",
    "top_n_list(data,'Description',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21aaea49-afe9-4633-8fc8-623eef986787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select('Category').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8905bc2-b397-44b8-b8bf-21d42cd779b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 614020\n",
      "Test Dataset Count: 264029\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = data.randomSplit([0.7,0.3], seed=60)\n",
    "print(\"Training Dataset Count:\", data_train.count())\n",
    "print(\"Test Dataset Count:\", data_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bce130b-7627-4b46-a9f2-89151e0a000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, OneHotEncoder, StringIndexer, VectorAssembler, HashingTF, IDF, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes \n",
    "\n",
    "#----------------Define tokenizer with regextokenizer()------------------\n",
    "regex_tokenizer = RegexTokenizer(pattern='\\\\W')\\\n",
    "                  .setInputCol(\"Description\")\\\n",
    "                  .setOutputCol(\"tokens\")\n",
    "\n",
    "#----------------Define stopwords with stopwordsremover()---------------------\n",
    "extra_stopwords = ['http','amp','rt','t','c','the']\n",
    "stopwords_remover = StopWordsRemover()\\\n",
    "                    .setInputCol('tokens')\\\n",
    "                    .setOutputCol('filtered_words')\\\n",
    "                    .setStopWords(extra_stopwords)\n",
    "                    \n",
    "\n",
    "#----------Define bags of words using countVectorizer()---------------------------\n",
    "count_vectors = CountVectorizer()\\\n",
    "               .setInputCol(\"filtered_words\")\\\n",
    "               .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "\n",
    "#-----------Encode the Category variable into label using StringIndexer-----------\n",
    "label_string_idx = StringIndexer()\\\n",
    "                  .setInputCol(\"Category\")\\\n",
    "                  .setOutputCol(\"label\")\n",
    "\n",
    "#-----------Define classifier structure for logistic Regression--------------\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "def metrics_ev(labels, metrics):\n",
    "    '''\n",
    "    List of all performance metrics\n",
    "    '''\n",
    "    # Confusion matrix\n",
    "    print(\"---------Confusion matrix-----------------\")\n",
    "    print(metrics.confusionMatrix)\n",
    "    print(' ')    \n",
    "    # Overall statistics\n",
    "    print('----------Overall statistics-----------')\n",
    "    print(\"Precision = %s\" %  metrics.precision())\n",
    "    print(\"Recall = %s\" %  metrics.recall())\n",
    "    print(\"F1 Score = %s\" % metrics.fMeasure())\n",
    "    print(' ')\n",
    "    # Statistics by class\n",
    "    print('----------Statistics by class----------')\n",
    "    for label in sorted(labels):\n",
    "       print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "       print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "       print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "    print(' ')\n",
    "    # Weighted stats\n",
    "    print('----------Weighted stats----------------')\n",
    "    print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "    print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "    print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "    print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "    print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afdb3ef-2ecd-47c5-9072-a64f4df6d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pipeline_cv_lr = Pipeline().setStages([regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, lr])\n",
    "model_cv_lr = pipeline_cv_lr.fit(data_train)\n",
    "predictions_cv_lr = model_cv_lr.transform(data_test)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time()\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f734b34f-7cc5-4803-99b1-844328ead2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/12 16:51:59 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Best Parameters: {Param(parent='RegexTokenizer_6728291e2d16', name='gaps', doc='whether regex splits on gaps (True) or matches tokens (False)'): True, Param(parent='RegexTokenizer_6728291e2d16', name='minTokenLength', doc='minimum token length (>= 0)'): 1, Param(parent='RegexTokenizer_6728291e2d16', name='outputCol', doc='output column name.'): 'tokens', Param(parent='RegexTokenizer_6728291e2d16', name='pattern', doc='regex pattern (Java dialect) used for tokenizing'): '\\\\W', Param(parent='RegexTokenizer_6728291e2d16', name='toLowercase', doc='whether to convert all characters to lowercase before tokenizing'): True, Param(parent='RegexTokenizer_6728291e2d16', name='inputCol', doc='input column name.'): 'Description'}\n",
      "Fold 1 - Elapsed Time: 712.86 seconds\n",
      "\n",
      "Training Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x13fb7b740>             \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/ml/wrapper.py\", line 53, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "                                              ^^^^^^^^^^^^^^\n",
      "AttributeError: 'CountVectorizer' object has no attribute '_java_obj'\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Best Parameters: {Param(parent='RegexTokenizer_6728291e2d16', name='gaps', doc='whether regex splits on gaps (True) or matches tokens (False)'): True, Param(parent='RegexTokenizer_6728291e2d16', name='minTokenLength', doc='minimum token length (>= 0)'): 1, Param(parent='RegexTokenizer_6728291e2d16', name='outputCol', doc='output column name.'): 'tokens', Param(parent='RegexTokenizer_6728291e2d16', name='pattern', doc='regex pattern (Java dialect) used for tokenizing'): '\\\\W', Param(parent='RegexTokenizer_6728291e2d16', name='toLowercase', doc='whether to convert all characters to lowercase before tokenizing'): True, Param(parent='RegexTokenizer_6728291e2d16', name='inputCol', doc='input column name.'): 'Description'}\n",
      "Fold 2 - Elapsed Time: 679.96 seconds\n",
      "\n",
      "Training Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Best Parameters: {Param(parent='RegexTokenizer_6728291e2d16', name='gaps', doc='whether regex splits on gaps (True) or matches tokens (False)'): True, Param(parent='RegexTokenizer_6728291e2d16', name='minTokenLength', doc='minimum token length (>= 0)'): 1, Param(parent='RegexTokenizer_6728291e2d16', name='outputCol', doc='output column name.'): 'tokens', Param(parent='RegexTokenizer_6728291e2d16', name='pattern', doc='regex pattern (Java dialect) used for tokenizing'): '\\\\W', Param(parent='RegexTokenizer_6728291e2d16', name='toLowercase', doc='whether to convert all characters to lowercase before tokenizing'): True, Param(parent='RegexTokenizer_6728291e2d16', name='inputCol', doc='input column name.'): 'Description'}\n",
      "Fold 3 - Elapsed Time: 704.53 seconds\n",
      "\n",
      "Training Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Best Parameters: {Param(parent='RegexTokenizer_6728291e2d16', name='gaps', doc='whether regex splits on gaps (True) or matches tokens (False)'): True, Param(parent='RegexTokenizer_6728291e2d16', name='minTokenLength', doc='minimum token length (>= 0)'): 1, Param(parent='RegexTokenizer_6728291e2d16', name='outputCol', doc='output column name.'): 'tokens', Param(parent='RegexTokenizer_6728291e2d16', name='pattern', doc='regex pattern (Java dialect) used for tokenizing'): '\\\\W', Param(parent='RegexTokenizer_6728291e2d16', name='toLowercase', doc='whether to convert all characters to lowercase before tokenizing'): True, Param(parent='RegexTokenizer_6728291e2d16', name='inputCol', doc='input column name.'): 'Description'}\n",
      "Fold 4 - Elapsed Time: 708.91 seconds\n",
      "\n",
      "Training Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Best Parameters: {Param(parent='RegexTokenizer_6728291e2d16', name='gaps', doc='whether regex splits on gaps (True) or matches tokens (False)'): True, Param(parent='RegexTokenizer_6728291e2d16', name='minTokenLength', doc='minimum token length (>= 0)'): 1, Param(parent='RegexTokenizer_6728291e2d16', name='outputCol', doc='output column name.'): 'tokens', Param(parent='RegexTokenizer_6728291e2d16', name='pattern', doc='regex pattern (Java dialect) used for tokenizing'): '\\\\W', Param(parent='RegexTokenizer_6728291e2d16', name='toLowercase', doc='whether to convert all characters to lowercase before tokenizing'): True, Param(parent='RegexTokenizer_6728291e2d16', name='inputCol', doc='input column name.'): 'Description'}\n",
      "Fold 5 - Elapsed Time: 668.36 seconds\n",
      "\n",
      "Average Time Across Folds: 694.92 seconds\n"
     ]
    }
   ],
   "source": [
    "lr_pipeline = Pipeline().setStages([regex_tokenizer, stopwords_remover, count_vectors, label_string_idx, lr])\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(count_vectors.vocabSize, [5000, 10000, 15000]) \\\n",
    "    .addGrid(count_vectors.minDF, [3, 5, 7]) \\\n",
    "    .addGrid(lr.maxIter, [10, 20, 30]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.3, 0.5]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=5)  \n",
    "fold_times = []\n",
    "\n",
    "for i in range(5):  \n",
    "    print(f\"\\nTraining Fold {i + 1}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    cv_model = crossval.fit(df)  \n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    fold_times.append(elapsed_time)\n",
    "\n",
    "    print(f\"Fold {i + 1} - Best Parameters: {cv_model.bestModel.stages[0].extractParamMap()}\")\n",
    "    print(f\"Fold {i + 1} - Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "$\n",
    "average_time = sum(fold_times) / len(fold_times)\n",
    "print(f\"\\nAverage Time Across Folds: {average_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bf528a8-ad9c-4124-b803-44464e288b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = cv_model.bestModel.transform(data_train)\n",
    "test_results = cv_model.bestModel.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9eeae1-f256-41ae-9f06-56c498d9a9c8",
   "metadata": {},
   "source": [
    "Train predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1343057d-238c-4f39-9d95-6eb487628338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------Check Top 5 predictions----------------------------------\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6705:=====>                                                 (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|                   Description|     Category|                   probability|label|prediction|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8091214496699353,0.02988...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8091214496699353,0.02988...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8091214496699353,0.02988...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8091214496699353,0.02988...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8091214496699353,0.02988...|  0.0|       0.0|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('-----------------------------Check Top 5 predictions----------------------------------')\n",
    "print(' ')\n",
    "train_results.select('Description','Category',\"probability\",\"label\",\"prediction\")\\\n",
    "                                        .orderBy(\"probability\", ascending=False)\\\n",
    "                                        .show(n=5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3230b2c2-8819-4a37-90b0-f72a655dd20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------Check Top 5 predictions----------------------------------\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6706:>                                                     (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|                   Description|     Category|                   probability|label|prediction|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8091214496699353,0.02988...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8091214496699353,0.02988...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8091214496699353,0.02988...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8091214496699353,0.02988...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8091214496699353,0.02988...|  0.0|       0.0|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('-----------------------------Check Top 5 predictions----------------------------------')\n",
    "print(' ')\n",
    "test_results.select('Description','Category',\"probability\",\"label\",\"prediction\")\\\n",
    "                                        .orderBy(\"probability\", ascending=False)\\\n",
    "                                        .show(n=5, truncate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db69a073-dae0-43b4-b7ce-be209f43e589",
   "metadata": {},
   "source": [
    "Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32ee153c-7507-40ee-9133-c78c8323bb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6707:=====>                                                 (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "------------------------------Accuracy----------------------------------\n",
      " \n",
      "                       accuracy:0.9344596882969053:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator_cv_lr = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(train_results)\n",
    "print(' ')\n",
    "print('------------------------------Accuracy----------------------------------')\n",
    "print(' ')\n",
    "print('                       accuracy:{}:'.format(evaluator_cv_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a925d-fff7-4974-99c8-866e0759d5cc",
   "metadata": {},
   "source": [
    "Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d3db852-6a30-46ea-86c5-5721f0672a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6709:=====>                                                 (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "------------------------------Accuracy----------------------------------\n",
      " \n",
      "                       accuracy:0.9343702864245909:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator_cv = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(test_results)\n",
    "print(' ')\n",
    "print('------------------------------Accuracy----------------------------------')\n",
    "print(' ')\n",
    "print('                       accuracy:{}:'.format(evaluator_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15a4b6-8d8b-4680-8def-09b24786235b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
