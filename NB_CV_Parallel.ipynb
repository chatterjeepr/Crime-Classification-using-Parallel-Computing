{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VKakVEOeyyR",
        "outputId": "82e3e821-9546-415c-d99f-7df9d6c37ce8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "UsV0akBqSB5E"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "spark = pyspark.sql.SparkSession.builder.appName(\"clipper-pyspark\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "IET_skWtfLbU"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "np.random.seed(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "A-w9hNNAmQhZ"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLg9UQTFfPlQ",
        "outputId": "6e319925-027c-4ef2-e6a8-82f79a9d801e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,X,Y\r\n",
            "5/13/15 23:53,WARRANTS,WARRANT ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",OAK ST / LAGUNA ST,-122.4258917,37.7745986\r\n",
            "5/13/15 23:53,OTHER OFFENSES,TRAFFIC VIOLATION ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",OAK ST / LAGUNA ST,-122.4258917,37.7745986\r\n",
            "5/13/15 23:33,OTHER OFFENSES,TRAFFIC VIOLATION ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",VANNESS AV / GREENWICH ST,-122.424363,37.80041432\r\n",
            "5/13/15 23:30,LARCENY/THEFT,GRAND THEFT FROM LOCKED AUTO,Wednesday,NORTHERN,NONE,1500 Block of LOMBARD ST,-122.4269953,37.80087263\r\n"
          ]
        }
      ],
      "source": [
        "%%sh\n",
        "#Let see the first 5 rows\n",
        "head -5 /train.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLQ1Z142iB7j",
        "outputId": "927cbd56-a05b-4293-beea-bce13976a0cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataframe Structure\n",
            "----------------------------------\n",
            "root\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            "\n",
            "None\n",
            " \n",
            "Dataframe preview\n",
            "+--------------+--------------------+\n",
            "|      Category|         Description|\n",
            "+--------------+--------------------+\n",
            "|      warrants|      warrant arrest|\n",
            "|other offenses|traffic violation...|\n",
            "|other offenses|traffic violation...|\n",
            "| larceny/theft|grand theft from ...|\n",
            "| larceny/theft|grand theft from ...|\n",
            "+--------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "None\n",
            " \n",
            "----------------------------------\n",
            "Total number of rows 878049\n"
          ]
        }
      ],
      "source": [
        "#Read the data into spark datafrome\n",
        "from pyspark.sql.functions import col, lower\n",
        "df = spark.read.format('csv')\\\n",
        "          .option('header','true')\\\n",
        "          .option('inferSchema', 'true')\\\n",
        "          .option('timestamp', 'true')\\\n",
        "          .load('/train.csv')\n",
        "\n",
        "data = df.select(lower(col('Category')),lower(col('Descript')))\\\n",
        "        .withColumnRenamed('lower(Category)','Category')\\\n",
        "        .withColumnRenamed('lower(Descript)', 'Description')\n",
        "data.cache()\n",
        "print('Dataframe Structure')\n",
        "print('----------------------------------')\n",
        "print(data.printSchema())\n",
        "print(' ')\n",
        "print('Dataframe preview')\n",
        "print(data.show(5))\n",
        "print(' ')\n",
        "print('----------------------------------')\n",
        "print('Total number of rows', df.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsNyYlWFiJjj",
        "outputId": "376738cd-e1cd-43db-b52b-870be8de783a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of unique value of Category: 39\n",
            " \n",
            "Top 10 Crime Category\n",
            "+--------------+----------+\n",
            "|      Category|totalValue|\n",
            "+--------------+----------+\n",
            "| larceny/theft|    174900|\n",
            "|other offenses|    126182|\n",
            "|  non-criminal|     92304|\n",
            "|       assault|     76876|\n",
            "| drug/narcotic|     53971|\n",
            "| vehicle theft|     53781|\n",
            "|     vandalism|     44725|\n",
            "|      warrants|     42214|\n",
            "|      burglary|     36755|\n",
            "|suspicious occ|     31414|\n",
            "+--------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            " \n",
            " \n",
            "Total number of unique value of Description: 879\n",
            " \n",
            "Top 10 Crime Description\n",
            "+--------------------+----------+\n",
            "|         Description|totalValue|\n",
            "+--------------------+----------+\n",
            "|grand theft from ...|     60022|\n",
            "|       lost property|     31729|\n",
            "|             battery|     27441|\n",
            "|   stolen automobile|     26897|\n",
            "|drivers license, ...|     26839|\n",
            "|      warrant arrest|     23754|\n",
            "|suspicious occurr...|     21891|\n",
            "|aided case, menta...|     21497|\n",
            "|petty theft from ...|     19771|\n",
            "|malicious mischie...|     17789|\n",
            "+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def top_n_list(df,var, N):\n",
        "    '''\n",
        "    This function determine the top N numbers of the list\n",
        "    '''\n",
        "    print(\"Total number of unique value of\"+' '+var+''+':'+' '+str(df.select(var).distinct().count()))\n",
        "    print(' ')\n",
        "    print('Top'+' '+str(N)+' '+'Crime'+' '+var)\n",
        "    df.groupBy(var).count().withColumnRenamed('count','totalValue')\\\n",
        "    .orderBy(col('totalValue').desc()).show(N)\n",
        "\n",
        "\n",
        "top_n_list(data, 'Category',10)\n",
        "print(' ')\n",
        "print(' ')\n",
        "top_n_list(data,'Description',10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YkoN42OiQ28",
        "outputId": "cf7b12b3-b175-4f4d-a87e-88b68cbce662"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.select('Category').distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZgRF-ShiU35",
        "outputId": "a8cdbf24-6357-4121-da6d-1db79a1fcb58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset Count: 614687\n",
            "Test Dataset Count: 263362\n"
          ]
        }
      ],
      "source": [
        "training, test = data.randomSplit([0.7,0.3], seed=60)\n",
        "#trainingSet.cache()\n",
        "print(\"Training Dataset Count:\", training.count())\n",
        "print(\"Test Dataset Count:\", test.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "f5gztk8_i1sd"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, OneHotEncoder, StringIndexer, VectorAssembler, HashingTF, IDF, Word2Vec\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "pN9mpwiHi2kU"
      },
      "outputs": [],
      "source": [
        "#----------------Define tokenizer with regextokenizer()------------------\n",
        "regex_tokenizer = RegexTokenizer(pattern='\\\\W')\\\n",
        "                  .setInputCol(\"Description\")\\\n",
        "                  .setOutputCol(\"tokens\")\n",
        "\n",
        "#----------------Define stopwords with stopwordsremover()---------------------\n",
        "extra_stopwords = ['http','amp','rt','t','c','the']\n",
        "stopwords_remover = StopWordsRemover()\\\n",
        "                    .setInputCol('tokens')\\\n",
        "                    .setOutputCol('filtered_words')\\\n",
        "                    .setStopWords(extra_stopwords)\n",
        "\n",
        "\n",
        "#----------Define bags of words using countVectorizer()---------------------------\n",
        "count_vectors = CountVectorizer(vocabSize=10000, minDF=5)\\\n",
        "               .setInputCol(\"filtered_words\")\\\n",
        "               .setOutputCol(\"features\")\n",
        "\n",
        "\n",
        "#-----------Using TF-IDF to vectorise features instead of countVectoriser-----------------\n",
        "hashingTf = HashingTF(numFeatures=10000)\\\n",
        "            .setInputCol(\"filtered_words\")\\\n",
        "            .setOutputCol(\"raw_features\")\n",
        "\n",
        "#Use minDocFreq to remove sparse terms\n",
        "idf = IDF(minDocFreq=5)\\\n",
        "        .setInputCol(\"raw_features\")\\\n",
        "        .setOutputCol(\"features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "F7NpwuSGjCGG"
      },
      "outputs": [],
      "source": [
        "#-----------Encode the Category variable into label using StringIndexer-----------\n",
        "label_string_idx = StringIndexer()\\\n",
        "                  .setInputCol(\"Category\")\\\n",
        "                  .setOutputCol(\"label\")\n",
        "\n",
        "#---------Define classifier structure for Naive Bayes----------\n",
        "nb = NaiveBayes(smoothing=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "cc4a_E27jGIH"
      },
      "outputs": [],
      "source": [
        "def metrics_ev(labels, metrics):\n",
        "    '''\n",
        "    List of all performance metrics\n",
        "    '''\n",
        "    # Confusion matrix\n",
        "    print(\"---------Confusion matrix-----------------\")\n",
        "    print(metrics.confusionMatrix)\n",
        "    print(' ')\n",
        "    # Overall statistics\n",
        "    print('----------Overall statistics-----------')\n",
        "    print(\"Precision = %s\" %  metrics.precision())\n",
        "    print(\"Recall = %s\" %  metrics.recall())\n",
        "    print(\"F1 Score = %s\" % metrics.fMeasure())\n",
        "    print(' ')\n",
        "    # Statistics by class\n",
        "    print('----------Statistics by class----------')\n",
        "    for label in sorted(labels):\n",
        "       print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
        "       print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
        "       print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
        "    print(' ')\n",
        "    # Weighted stats\n",
        "    print('----------Weighted stats----------------')\n",
        "    print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
        "    print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
        "    print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
        "    print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
        "    print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t1SybbBjhwS",
        "outputId": "9c29f02f-d31e-4008-b9b3-f39dfceb61e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "--------------------------Accuracy-----------------------------\n",
            " \n",
            "                      accuracy:0.9938361301234255:\n",
            "Time taken for Model Prediction: 24.60 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "### Secondary model using NaiveBayes\n",
        "pipeline_cv_nb = Pipeline().setStages([regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, nb])\n",
        "model_cv_nb = pipeline_cv_nb.fit(training)\n",
        "predictions_cv_nb = model_cv_nb.transform(test)\n",
        "evaluator_cv_nb = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_cv_nb)\n",
        "print(' ')\n",
        "print('--------------------------Accuracy-----------------------------')\n",
        "print(' ')\n",
        "print('                      accuracy:{}:'.format(evaluator_cv_nb))\n",
        "end_time = time.time()\n",
        "elapsed_time_prediction2 = end_time - start_time\n",
        "# Print Time taken for Model Prediction\n",
        "print(f\"Time taken for Model Prediction: {elapsed_time_prediction2:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# Create a MulticlassClassificationEvaluator\n",
        "evaluator_idf_nb = MulticlassClassificationEvaluator(metricName='accuracy', labelCol='label', predictionCol='prediction')\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTf.numFeatures, [10000, 50000, 100000]) \\\n",
        "    .addGrid(idf.minDocFreq, [1, 5, 10]) \\\n",
        "    .addGrid(nb.smoothing, [0.1, 0.5, 1.0]) \\\n",
        "    .build()\n",
        "\n",
        "# Create a cross-validator\n",
        "crossval = CrossValidator(estimator=pipeline_idf_nb,\n",
        "                          estimatorParamMaps=param_grid,\n",
        "                          evaluator=evaluator_idf_nb,\n",
        "                          numFolds=3)\n",
        "\n",
        "# Fit the cross-validator to the data\n",
        "cv_model = crossval.fit(x_train['Descript'], y_train)\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "best_model = cv_model.bestModel\n",
        "predictions = best_model.transform(x_test['Descript'])\n",
        "\n",
        "# Evaluate the best model\n",
        "best_accuracy = evaluator_idf_nb.evaluate(predictions)\n",
        "\n",
        "print(' ')\n",
        "print('-----------------------------Best Model Accuracy-----------------------------')\n",
        "print(' ')\n",
        "print('                          accuracy:{}:'.format(best_accuracy))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
